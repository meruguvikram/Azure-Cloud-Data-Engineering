# DBFS

## Upload files into databricks

1. Click on `data` > under `upload file` > drag and drop file from rritec materials labdata/reatail_db/orders/part-00000

    ![image](https://user-images.githubusercontent.com/20516321/222360084-1a8ab005-c2d4-4721-8a04-207e6f917468.png)

2.  Copy the path `/FileStore/tables/part_00000-2`, it is needed in next exercise

## See the data of files in DBFS

    1. Click on **Create** > Click on **Notebook** > Name it as **Create Dataframes** > Click on **Create**
    2. Read about [Databricks Utilities](https://docs.databricks.com/dev-tools/databricks-utils.html#databricks-utilities)
        ``` fs
        dbutils.help()
        ```
        ``` fs
        dbutils.fs.help()
        ```

        ``` fs
        dbutils.fs.help("head")
        ```
        ``` fs
        dbutils.fs.head("/FileStore/tables/part_00000-2",100)
        ```

    3. type as for below
        ``` fs
        %fs head /FileStore/tables/part_00000-2
        ```
        ![image](https://user-images.githubusercontent.com/20516321/222361426-8b3fd531-c2ae-49a7-9b36-c97015c30f71.png)

        
    3. 
# Create Dataframes

## Create dataframe from csv file
    1. Create dataframe
        ``` sql
        orders_df = (spark.read.csv("/FileStore/tables/part_00000-2")
             .toDF("order_id", "order_date", "order_customer_id", "order_status")
            )
        ```
    2. observe type
        ``` sql
        type(orders_df)
        ```
    3. See some data
        ``` sql
        orders_df.show()
        ```
        ``` py
        orders_df.groupBy("order_status").count().show()
        ```

    4. 
## Create dataframe from json file
    1. upload order json file
        ![image](https://user-images.githubusercontent.com/20516321/222363656-9aee863c-a7d3-46d8-8837-eba913b6602e.png)

    2. Observe data
        ``` fs
        %fs head /FileStore/tables/part_r_00000_990f5773_9005_49ba_b670_631286032674-1
        ```
    3. Create dataframe
        ``` sql
        orders_json = spark.read.json("/FileStore/tables/part_r_00000_990f5773_9005_49ba_b670_631286032674-1")
        ```
    2. observe type
        ``` sql
        type(orders_json)
        ```
    3. See some data
        ``` sql
        orders_json.show()
        ```
    4. 

## Create dataframe from json file using format and load

    1. Create dataframe
        ``` sql
        orders_df_json = (spark.read.format("json")
               .load("/FileStore/tables/part_r_00000_990f5773_9005_49ba_b670_631286032674-1")
              )
        ```
    2. observe type
        ``` sql
        type(orders_df_json)
        ```
    3. See some data
        ``` sql
        orders_df_json.show(3)
        ```
    4. 
## DF Creation by enabling the header
    1. Refer spark help document [here](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)upload employee csv file       

    2. Observe data
        ``` fs
        %fs head  /FileStore/tables/nov2022/emp_csv/employee.csv
        ```
    3. Create dataframe
        ``` sql
        emp_df = (spark
          .read
          .format("csv")
          .option("header", "true")
          .load("/FileStore/tables/nov2022/emp_csv/employee.csv")
          )
        ```
    2. observe type
        ``` sql
        type(emp_df)
        ```
    3. See some data
        ``` sql
        emp_df.show(3)
        ```
    4
## DF Creation by using pipe (|) delimited data
    1. upload departments csv file       

    2. Observe data
        ``` fs
        %fs head /FileStore/tables/nov2022/dept_pipe/departments
        ```
    3. Create dataframe

       ``` sql
        dept_df = (spark
           .read
           .format("csv")
           .option("header", "true")
           .option("delimiter", "|") 
           .load("/FileStore/tables/nov2022/dept_pipe/departments")
           )
        ```
## Create dataframe with options
1. Create dataframe
        ``` sql
        dept_df = (spark
           .read
           .format("csv")
           .options(header=True,delimiter= "|") 
           .load("/FileStore/tables/nov2022/dept_pipe/departments")
           )
        ```
        
  
 

2. observe type
    ``` sql
    type(dept_df)
    ```
3. See some data
    ``` sql
    dept_df.show(3)
    ```

## Create dataframe from multiline/nested json file

1. upload students_nested_json_data file
2. Observe data
    ``` fs
    %fs head /FileStore/tables/nov2022/students_nested_json/students_nested_json_data.json
    ```
3. Create dataframe
    ``` sql
    stu_df = (spark
      .read
      .format("json")
      .option("multiline", "true")
      .load("/FileStore/tables/nov2022/students_nested_json/students_nested_json_data.json")
       )
    ```
2. observe type
    ``` sql
    type(stu_df)
    ```
3. See some data
    ``` sql
    stu_df.show(3)
    ```
4. 
