# Read Modes
1. mode 
    1. (default `PERMISSIVE`): allows a mode for dealing with corrupt records during parsing.
    2. `PERMISSIVE` : sets other fields to null when it meets a corrupted record, and puts the malformed string into a new field configured by                columnNameOfCorruptRecord. When a schema is set by user, it sets null for extra fields.
    3. `DROPMALFORMED` : ignores the whole corrupted records.
    4. `FAILFAST` : throws an exception when it meets corrupted records.
2. upload and observe the data
``` pyspark
%fs head /FileStore/tables/rritec/output/ford.json
```
2. create dataframe
``` pyspark
ford_df = (spark
           .read
           .format("json")
           .load("/FileStore/tables/rritec/output/ford.json"))
display(ford_df)
```
3. Observe number of records
``` pyspark
ford_df.count()
```
## Read Modes - PERMISSIVE
1. default mode is `PERMISSIVE`
``` pyspark
ford_df = (spark
           .read
           .format("json")
           .option("mode","PERMISSIVE")
           .load("/FileStore/tables/rritec/output/ford.json"))
display(ford_df)
```
## Read Modes - DROPMALFORMED
``` pyspark
ford_df = (spark
           .read
           .format("json")
           .option("mode","DROPMALFORMED")
           .load("/FileStore/tables/rritec/output/ford.json"))
display(ford_df)
```
``` pyspark
ford_df.count()
```
## Read Modes - FAILFAST
1. run below command and observe the error message
``` pyspark
ford_df = (spark
           .read
           .format("json")
           .option("mode","FAILFAST")
           .load("/FileStore/tables/rritec/output/ford.json"))
```
## badRecordsPath
1. Create bad records in one separate file
``` pyspark
ford_df = (spark
           .read
           .format("json")
           .option("badRecordsPath", "/FileStore/tables/rritec/output/ford_corrupted_records")
           .load("/FileStore/tables/rritec/output/ford.json"))
display(ford_df)
```
2. observe bad records
``` pyspark
%fs ls /FileStore/tables/rritec/output/ford_corrupted_records
```
## inferSchema

1. observe data
``` pyspark 
%fs head /FileStore/tables/rritec/output/department.csv
```
``` pyspark
dept_df = (spark
           .read
           .format("csv")
           .option("header", "true")
           .option("inferSchema", "true")           
           .load("/FileStore/tables/rritec/output/department.csv"))
```
``` pyspark
display(dept_df)
```
``` pyspark
dept_df.printSchema()
```
# Custom schema by using Struct Type
1. import required functions and creat schema
``` pyspark
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType
 
# StructType --> Row
# StructField --> Fields
 
dept_custom_schema = StructType([StructField("EmployeeID", IntegerType()),
                                StructField("DepartmentName", StringType()),
                                StructField("Client", StringType()),
                                StructField("OnboardedDate", DateType())])
```
                                
2. observe datatype
``` pysaprk
type(dept_custom_schema)
```
3. create dataframe
``` pyspark
dept_df = (spark
           .read
           .format("csv")
           .option("header", "true")
           .schema(dept_custom_schema)
           .load("/FileStore/tables/rritec/output/department.csv"))
```
4. display the contents
``` pysaprk
display(dept_df)
```
5. Observe the schema
``` pyspark
dept_df.printSchema()
```
# Custom Schema by using DDL String

1. observe the content
``` pysaprk
%fs head /FileStore/tables/rritec/output/orders_csv_latest/part_00000
```
2. Create DDL string
``` pyspark
orders_custom_schema = "order_id int, order_date date, order_customer_id int, order_status string"
```
3. create dataframe
``` pyspark
orders_df = (spark
             .read
             .format("csv")
             .schema(orders_custom_schema)
             .load("/FileStore/tables/rritec/output/orders_csv_latest/part_00000"))
display(orders_df)
```
4. observe schema
```pyspark
orders_df.printSchema()
```
# Joining 2 dataframes
1. already we have `dept_df`
``` pysaprk
display(dept_df)
```
2. create `emp_df`
``` pyspark
%fs head /FileStore/tables/rritec/output/emp_csv_latest/employee.csv
```
``` pyspark
emp_df = (spark
          .read
          .format("csv")
          .option("header", "true")
          .load("/FileStore/tables/rritec/output/emp_csv_latest/employee.csv"))
```
``` pyspark
display(emp_df)
```
``` pyspark
joined_df = dept_df.join(emp_df, emp_df.ID == dept_df.EmployeeID)
```
``` pyspark
display(joined_df)
```
3. 
# dropping the columns
1. drop `ID` column
``` pyspark
display(joined_df.drop("ID"))
```
2. observe data
``` pysaprk
display(joined_df)
```
3. create final_df
``` pysaprk
final_df = joined_df.drop("DepartmentName", "Client", "OnboardedDate")
```
``` pysaprk
display(final_df)
```
``` pysaprk
display(joined_df)
```
