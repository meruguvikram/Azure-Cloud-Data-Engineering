# Spark DF Functions

1. Create df
``` python

emp_data = [("James", "M", 60000),
           ("Maria", "F", 70000),
           ("Mike", None, 50000),
           ("Jen", "", None)]

columns = ["emp_name", "emp_gender", "emp_sal"]
emp_df = spark.createDataFrame(data = emp_data, schema = columns)
display(emp_df)

```
2. How to add one or more columns to the dataframe?
``` python

emp_df1=emp_df.withColumn("tax",emp_df.emp_sal *0.01).withColumn("hra",emp_df.emp_sal *0.03)
display(emp_df1)

```
3. case when
``` python

from pyspark.sql.functions import expr
new_df = emp_df1.withColumn("new_gender", expr("""CASE WHEN emp_gender = 'M' THEN 'Male'
                                                      WHEN emp_gender = 'F' THEN 'Female'
                                                      WHEN emp_gender IS NULL THEN ''
                                                      ELSE emp_gender
                                                  END"""))
display(new_df)

```
4. lit
``` python

from pyspark.sql.functions import lit
org_df = emp_df.withColumn("Organization", lit("Netflix"))
display(org_df)

```

``` python

from pyspark.sql.functions import lit
mgr_df = emp_df.withColumn("manager_id", lit(123))
display(mgr_df)

```

5. split
``` python

emp_data = [("james,a,smith", 123),
           ("steve,john,david", 456)]
columns = ["emp_full_name", "emp_id"]
emp_df = spark.createDataFrame(data = emp_data, schema = columns)
display(emp_df)

```
``` python
from pyspark.sql.functions import split
split_df = (
            emp_df
            .withColumn("emp_first_name", split("emp_full_name", ",")[0])
            .withColumn("emp_middle_name", split("emp_full_name", ",")[1])
            .withColumn("emp_last_name", split("emp_full_name", ",")[2])
        )
display(split_df)

```
6. concat_ws
```python

stu_data = [("steve", ["java", "scala"]),
           ("david", ["c", "python"])]
columns = ["stu_name", "languages_known"]
stu_df = spark.createDataFrame(data = stu_data, schema = columns)
display(stu_df)

```

``` python

stu_df.printSchema()

```

``` python

from pyspark.sql.functions import concat_ws
new_df = stu_df.withColumn("languages", concat_ws("#", "languages_known"))
display(new_df)

```

``` python

new_df.printSchema()

```
7. explode
``` python

display(stu_df)

```
``` python

stu_df.printSchema()

```
``` python

from pyspark.sql.functions import explode
explode_df = stu_df.withColumn("languages", explode("languages_known"))
display(explode_df)

```

8. withColumnRenamed
``` python

display(stu_df)

```
``` python

new_df = stu_df.withColumnRenamed("languages_known", "common_languages")
display(new_df)

```

``` python

new_df.printSchema()

```

9. Explode on a Nested Json

``` python

stu_df = (spark
          .read
          .format("json")
          .option("multiline", "true")
          .load("/FileStore/tables/June2022/stu_nested_json/students_nested_json_data.json"))
display(stu_df)

```
``` python

stu_df.printSchema()

```
```python

from pyspark.sql.functions import explode
explode_df = stu_df.withColumn("edu_flattened", explode("Education"))
display(explode_df)

```
``` python

dropped_df = explode_df.drop("Education")
display(dropped_df)

```
``` python

dropped_df.printSchema()

```

``` python

final_flattened_df = dropped_df.select("name", "edu_flattened.*")
display(final_flattened_df)

```
``` python

final_flattened_df.printSchema()

```
10.  collect_list
``` python

emp_data = [("James", "Sales", 3000),
("Michael", "Sales", 4600),
("Robert", "Sales", 4100),
("Maria", "Finance", 3000),
("James", "Sales", 3000),
("Scott", "Finance", 3300),
("Jen", "Finance", 3900),
("Jeff", "Marketing", 3000),
("Kumar", "Marketing", 3000),
("James", "Sales", 2000),
("Saif", "Sales", 4100)]

columns = ["emp_name", "emp_dept", "emp_sal"]

emp_df = spark.createDataFrame(data = emp_data, schema = columns)

display(emp_df)
```

``` python
from pyspark.sql.functions import collect_list
display(emp_df.select(collect_list("emp_sal").alias("emp_sal_list")))
```
11. collect_set
 ``` python
 
 from pyspark.sql.functions import collect_set
display(emp_df.select(collect_set("emp_sal").alias("emp_sal_set")))

```
12. count

``` python

from pyspark.sql.functions import count
display(emp_df.select(count("emp_sal").alias("emp_sal_count")))

```
13. countDistinct
``` python

from pyspark.sql.functions import countDistinct
display(emp_df.select(countDistinct("emp_sal").alias("emp_sal_distinct_count")))

```
14. first & last
``` python

from pyspark.sql.functions import first
display(emp_df.select(first("emp_sal").alias("emp_sal_first")))

```

``` python

from pyspark.sql.functions import  last
display(emp_df.select(last("emp_sal").alias("emp_sal_last")))

```

15. max & min
``` python

from pyspark.sql.functions import  max
display(emp_df.select(max("emp_sal").alias("emp_sal_max")))

```
``` python

from pyspark.sql.functions import  min
display(emp_df.select(min("emp_sal").alias("emp_sal_min")))

```

16. sum
``` python

from pyspark.sql.functions import  sum
display(emp_df.select(sum("emp_sal").alias("emp_sal_sum")))

```
17. sumDistinct
``` python

from pyspark.sql.functions import  sumDistinct
display(emp_df.select(sumDistinct("emp_sal").alias("emp_sal_sumDistinct")))

```
18. row_number
``` python

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window.partitionBy("emp_dept").orderBy(desc("emp_sal"))

```
``` python

from pyspark.sql.functions import row_number
new_df = emp_df.withColumn("row_number", row_number().over(windowSpec))
display(new_df)

```
``` python

final_df = new_df.filter("row_number == 1")
display(final_df)

```
``` python

display(new_df.filter("row_number <= 2"))

```
20. rank
``` python

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window.partitionBy("emp_dept").orderBy(desc("emp_sal"))

```
``` python

from pyspark.sql.functions import rank
new_df = emp_df.withColumn("rank", rank().over(windowSpec))
display(new_df)

```

``` python

final_df = new_df.filter("rank == 1")
display(final_df)

```
``` python

display(new_df.filter("rank <= 2"))

```
21. dense_rank
``` python

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window.partitionBy("emp_dept").orderBy(desc("emp_sal"))

```
``` python

from pyspark.sql.functions import dense_rank
new_df = emp_df.withColumn("dense_rank", dense_rank().over(windowSpec))
display(new_df)

```
``` python

final_df = new_df.filter("dense_rank == 1")
display(final_df)

```
``` python

display(new_df.filter("dense_rank <= 2"))

```
22. lag
``` python

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window.partitionBy("emp_dept").orderBy(desc("emp_sal"))

```
``` python

from pyspark.sql.functions import lag
new_df = emp_df.withColumn("lag", lag("emp_sal", 1).over(windowSpec))
display(new_df)
```
23. lead
``` python

from pyspark.sql.window import Window
from pyspark.sql.functions import desc
windowSpec = Window.partitionBy("emp_dept").orderBy(desc("emp_sal"))

```
``` python
from pyspark.sql.functions import lead
new_df = emp_df.withColumn("lead", lead("emp_sal", 1).over(windowSpec))
display(new_df)

```
25. partitionBy
``` python

emp_df.write

```
``` python

(emp_df
 .write
 .option("header", "true")
 .partitionBy("emp_dept")
 .format("csv")
 .save("/Filestore/tables/emp_csv_output_dec_2022"))
 
 ```
 
 ``` python
 
 %fs ls /Filestore/tables/emp_csv_output_dec_2022
 
 ```
 
 ``` python
 
 %fs ls dbfs:/Filestore/tables/emp_csv_output_dec_2022/emp_dept=Finance/
 
 ```
 ``` python
 
 %fs head dbfs:/Filestore/tables/emp_csv_output_dec_2022/emp_dept=Finance/part-00002-tid-7195863566135644683-8dcd1890-647d-4722-a2b8-ef7f6f8c0fcb-184-1.c000.csv
 
 ```
 
 ``` python
 
 %fs ls dbfs:/Filestore/tables/emp_csv_output_dec_2022/emp_dept=Marketing/
 
 ```
 
 ``` python
 
 %fs head dbfs:/Filestore/tables/emp_csv_output_dec_2022/emp_dept=Marketing/part-00005-tid-7195863566135644683-8dcd1890-647d-4722-a2b8-ef7f6f8c0fcb-187-2.c000.csv
 
 ```
