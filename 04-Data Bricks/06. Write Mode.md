# Write Mode
1. Write modes would be used to write `Spark DataFrame` as `JSON, CSV, Parquet, Avro, ORC, Text files` and also used to write to `Hive table`, `JDBC tables like MySQL, SQL server`, e.t.c


| Scala/Java |	Any Language |	Meaning |
| ------- | ------ | ------- |
| SaveMode.ErrorIfExists (default) | "error" or "errorifexists" (default) | When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown. |
| SaveMode.Append	| "append"	| When saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data. |
| SaveMode.Overwrite	| "overwrite"	| Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame. |
| SaveMode.Ignore	| "ignore"	| Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL. | 

2. Refer Modes [here](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes)

1. create a simple dataframe
``` python

emp_data = [("James", "M", 60000),
           ("Maria", "F", 70000),
           ("Mike", None, 50000),
           ("Jen", "", None)]

emp_columns = ["emp_name", "emp_gender", "emp_sal"]
df = spark.createDataFrame(data = emp_data, schema = emp_columns)
display(df)

```
2. Saving the content of dataframe into external storage.
``` pyspark
df.write.format("json").save("/FileStore/tables/rritec/output/df_data")
```
``` pyspark
%fs ls /FileStore/tables/rritec/output/df_data
```
2. Copy the json file dbfs path and observe contents
``` pyspark
%fs head dbfs:/FileStore/tables/rritec/output/df_data/part-00000-tid-766577745249667303-c54f173e-99aa-4f7b-b754-2a8a092de2a9-4-1-c000.json
```
3. What happenes if we write once again

``` pyspark
df.write.format("json").save("/FileStore/tables/rritec/output/df_data")
```
## Write Modes - error / errorifexists
1. Add `error` mode and observe it
``` pyspark
(df
 .write
 .format("json")
 .mode("error")
 .save("/FileStore/tables/rritec/output/df_data"))
 ```
2. Add `errorifexists` mode and observe it
``` pyspark
(df
 .write
 .format("json")
 .mode("errorifexists")
 .save("/FileStore/tables/rritec/output/df_data"))
 ```
## Write modes - ignore
1. Add `error` mode and observe it
``` pyspark
(df
 .write
 .format("json")
 .mode("ignore")
 .save("/FileStore/tables/rritec/output/df_data"))
 ```
 ## Write modes - append
1. Add `error` mode and observe it
``` pyspark
(df
 .write
 .format("json")
 .mode("append")
 .save("/FileStore/tables/rritec/output/df_data"))
 ```
 ``` pyspark
 %fs ls /FileStore/tables/rritec/output/df_data
 ```
 ``` pyspark
 %fs head dbfs:/FileStore/tables/rritec/output/df_data/part-00000-tid-7383138309953482542-3d9411b4-16ae-45e7-afbb-843a0c80f621-5-1-c000.json
 ```
  ``` pyspark
 %fs head dbfs:/FileStore/tables/rritec/output/df_data/part-00000-tid-766577745249667303-c54f173e-99aa-4f7b-b754-2a8a092de2a9-4-1-c000.json
 ```
 ## Write modes - overwrite
1. Add `error` mode and observe it
``` pyspark
(df
 .write
 .format("json")
 .mode("overwrite")
 .save("/FileStore/tables/rritec/output/df_data"))
 ```
``` pyspark
 %fs ls /FileStore/tables/rritec/output/df_data
 ```
## Adding header when writing the DF
1. Write into csv file
``` pyspark
(final_df
 .write
 .format("csv")
 .mode("overwrite")
 .option("header", "true")
 .save("/FileStore/tables/rritec/output/df_csv_data")
 )
```
``` pyspark
%fs ls /FileStore/tables/rritec/output/final_student_csv_data
```
```pysaprk
%fs head dbfs:/FileStore/tables/rritec/output/final_student_csv_data/part-00000-tid-4721931960538371143-1b0b3268-03e9-4725-9d83-269e16786577-8-1-c000.csv
```
## Write a Single file using Spark coalesce() & repartition()
1. When you are ready to write a DataFrame, first use Spark repartition() and coalesce() to merge data from all partitions into a single partition and then save it to a file. This still creates a directory and write a single part file inside a directory instead of multiple part files
2. Both coalesce() and repartition() are Spark Transformation operations that shuffle the data from multiple partitions into a single partition. **Use coalesce() as it performs better and uses lesser resources compared with repartition().**
3. You have to be very careful when using Spark coalesce() and repartition() methods on larger datasets as they are expensive operations and could throw OutOfMemory errors.
``` python
(df 
 .coalesce(1)
 .write
 .format("csv")
 .mode("overwrite")
 .save("dbfs:/FileStore/b2404052/")
)
```
``` python
(df 
 .repartition(1)
 .write
 .format("csv")
 .mode("overwrite")
 .save("dbfs:/FileStore/b2404053/")
)
```

## Questions
## Answers
